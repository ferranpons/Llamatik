<p align="center">
  <img src="https://raw.githubusercontent.com/ferranpons/llamatik/main/assets/llamatik-logo.png" alt="Llamatik Logo" width="200"/>
</p>

<h1 align="center">ğŸ¦™ Llamatik</h1>

<p align="center">
  Kotlin-first llama.cpp integration for on-device and remote LLM inference.
</p>

<p align="center"><i>Kotlin. LLMs. On your terms.</i></p>

---

## ğŸš€ Features

- âœ… Kotlin Multiplatform: shared code across Android, iOS, and desktop
- âœ… Offline inference via llama.cpp (compiled with Kotlin/Native bindings)
- âœ… Remote inference via optional HTTP client (e.g. llama-cpp-server)
- âœ… Embeddings and text generation support
- âœ… Works with GGUF models (e.g. Mistral, Phi, LLaMA)
- âœ… Lightweight and dependency-free runtime

---

## ğŸ”§ Use Cases

- ğŸ§  On-device chatbots
- ğŸ“š Local RAG systems
- ğŸ›°ï¸ Hybrid AI apps with fallback to remote LLMs
- ğŸ® Game AI, assistants, and dialogue generators

---

## ğŸ§± Architecture

Llamatik provides two core modules:

- `llamatik-core`: Native C++ llama.cpp integration via Kotlin/Native
- `llamatik-client`: Lightweight HTTP client to connect to remote llama.cpp-compatible backends

All backed by a shared Kotlin API so you can switch between local and remote seamlessly.

---

## ğŸ“¦ Installation

Coming soon â€” stay tuned for full Gradle instructions and prebuilt binaries.

---

## ğŸ¤ Contributing

Llamatik is 100% open-source and actively developed. Contributions, bug reports, and feature
suggestions are welcome!

---

## ğŸ“œ License

[MIT](./LICENSE)

---

Built with â¤ï¸ for the Kotlin community.
