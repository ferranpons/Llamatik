---
title: â€œIntroducing Llamatik Offline LLMs for Kotlin Multiplatformâ€
date: 2025-07-22T09:00:00-05:00
description: â€œMeet Llamatik â€” a multiplatform Kotlin library to run llama.cpp locally on Android, iOS, and desktop, complete with an HTTP server.â€
tags: ["Kotlin", "Multiplatform", "LLM", "llama.cpp", "Offline AI"]
showDate: true
draft: false
---

ğŸ¦™ Introducing Llamatik: Offline LLMs for Kotlin Multiplatform

Weâ€™re thrilled to introduce Llamatik â€” an open-source Kotlin Multiplatform library that brings local
Large Language Models (LLMs) to Android, iOS, desktop, and beyond using the power of llama.cpp.

Llamatik makes it simple and efficient to integrate offline, on-device inference and embeddings into
your KMP apps, whether youâ€™re building an AI assistant, a RAG chatbot, or an edge intelligence tool.

â¸»

âœ¨ Why Llamatik?

While the AI ecosystem is rich with APIs, most Kotlin developers are still tied to cloud-based
models. That means latency, privacy risks, and ongoing costs.

We believe the future of AI is:

â€¢ ğŸ” Private: Your data stays on-device.

â€¢ ğŸ“± Multiplatform: One codebase for Android, iOS, macOS, Linux, and Windows.

â€¢ âš¡ Performant: Built for small, quantized models that run fast on consumer hardware.

Llamatik bridges the gap between llama.cppâ€™s C++ backend and Kotlinâ€™s modern multiplatform tooling â€”
giving you total control over your models and your data.

â¸»

ğŸ’¡ What Can You Do with It?

With Llamatik, you can:

â€¢ ğŸ§  Run quantized LLMs like Phi-2, Mistral, or TinyLlama completely offline.

â€¢ ğŸ” Generate embeddings using models like nomic-embed-text or bge-small locally.

â€¢ âš¡ Launch your own HTTP inference server using Ktor â€” fully self-contained, built into Llamatik,
and powered by llama.cpp.

â€¢ ğŸŒ Connect to remote llama.cpp endpoints (like llama-cpp-python, llama-server, or Ollama) with the
bundled HTTP client.

â€¢ ğŸ” Use the same API across Android, iOS, and other native platforms â€” no platform-specific code
needed.

â¸»

ğŸŒ Built-in HTTP Inference Server

Llamatik includes a ready-to-use Ktor-based HTTP server that wraps your local llama.cpp models. You
can spin it up with a single call and expose endpoints like:

POST /v1/chat/completions

POST /v1/embeddings

GET /v1/models

This makes it easy to:

â€¢ ğŸ”— Connect your own apps (or other devices) to a shared local model

â€¢ ğŸ§ª Use OpenAI-compatible tooling (e.g. LangChain, LlamaIndex) with your local server

â€¢ âš™ï¸ Integrate into your own edge deployments or experiments

No Python needed. No Docker required. Just Kotlin + Llamatik.

â¸»

ğŸ“¦ Whatâ€™s Inside?

â€¢ ğŸ¦™ Native bindings to llama.cpp for Kotlin/Native targets (no JNI or JNI-only limitations).

â€¢ ğŸ§  Multi-model context manager (for simultaneous generation + embeddings).

â€¢ ğŸ›°ï¸ Optional HTTP client and server.

â€¢ ğŸ§± Model loading, prompt building, and memory-efficient execution tailored for mobile.

â€¢ ğŸ› ï¸ Simple, extensible API â€” built for Kotlin, not adapted from Python.

â¸»

ğŸ“¢ Get Involved

Llamatik is open-source and community-driven. Whether youâ€™re building AI-first apps or simply
exploring whatâ€™s possible offline, weâ€™d love to hear from you.

ğŸ”— [GitHub Repo](https://github.com/ferranpons/Llamatik)