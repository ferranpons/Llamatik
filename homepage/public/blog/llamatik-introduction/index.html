<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
    


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="Kotlin-first llama.cpp integration for on-device and remote LLM inference.">
<title>
    “ Introducing Llamatik Offline Llms for Kotlin Multiplatform” - Llamatik
</title>



    
    <meta property="og:title"
      content="“Introducing Llamatik Offline LLMs for Kotlin Multiplatform” - Llamatik"/>
<meta property="og:type" content="website"/>
<meta property="og:description"
      content="“Meet Llamatik — a multiplatform Kotlin library to run llama.cpp locally on Android, iOS, and desktop, complete with an HTTP server.”"/>
<meta property="og:url" content="http://localhost:1313/blog/llamatik-introduction/"/>
<meta property="og:site_name" content="Llamatik"/>




<meta property="og:image" content="http://localhost:1313/home/llamatik-icon-logo.png"/>

<meta property="og:image" content="http://localhost:1313/home/profile.jpg"/>




    
<link rel="shortcut icon" href="/img/fav.ico">


    




<link rel="stylesheet" href="/css/main.css" media="screen">





    
    
    
    
</head>
<body>
<section id="top" class="section">
    
    <div class="container hero  fade-in one ">
        

<h1 class="bold-title is-1">Llamatik Blog</h1>


    </div>
    
    <div class="section  fade-in two ">
        

<div class="container">
    <hr>
    <nav class="navbar" role="navigation" aria-label="main navigation">
        
        <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu"
           aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
        <div class="navbar-menu " id="navMenu">
            
            
            
            
            <a class="navbar-item" href="/">main</a>
            

            
            

            
            
            

            

            
            
            
            
            
            <a class="navbar-item"
               href="/%20#about">About</a>
            
            
            
            
            
            
            
            
            
            <a class="navbar-item" href="http://localhost:1313/blog/">
                
                Back to Llamatik Blog
                
            </a>
            
            
            
            
            
            

            
            
            <a class="navbar-item"
               href="/%20#contact">Contact</a>
            
            

            
            
            

            
            <a class="navbar-item" href=""><i class="fas fa-rss"></i></a>
            
            
        </div>
    </nav>
    <hr>
</div>




        
<div class="container">
    <h2 class="title is-1 top-pad strong-post-title">
        <a href="http://localhost:1313/blog/llamatik-introduction/">“Introducing Llamatik Offline LLMs for Kotlin Multiplatform”</a>
    </h2>
    
    <div class="post-data">
        Jul 22, 2025
        
         | 
        3 minutes read
        
    </div>
    
    <div class="blog-share">
        Share this:
        
        <a class="twitter-share-button"
           href="https://twitter.com/intent/tweet?text=%e2%80%9cIntroducing%20Llamatik%20Offline%20LLMs%20for%20Kotlin%20Multiplatform%e2%80%9d%20http%3a%2f%2flocalhost%3a1313%2fblog%2fllamatik-introduction%2f"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
            <i class="fab fa-twitter"></i>
            <span class="hidden">Twitter</span>
        </a>
        
        
        
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <p>
        Tags:
        
        <a href="/tags/kotlin">
        Kotlin</a>,
        
        <a href="/tags/multiplatform">
        Multiplatform</a>,
        
        <a href="/tags/llm">
        LLM</a>,
        
        <a href="/tags/llama.cpp">
        llama.cpp</a>,
        
        <a href="/tags/offline-ai">
        Offline AI</a>
        
    </p>
    
    
    
    
</div>

<div class="container markdown top-pad">
    <p>🦙 Introducing Llamatik: Offline LLMs for Kotlin Multiplatform</p>
<p>We’re thrilled to introduce Llamatik — an open-source Kotlin Multiplatform library that brings local
Large Language Models (LLMs) to Android, iOS, desktop, and beyond using the power of llama.cpp.</p>
<p>Llamatik makes it simple and efficient to integrate offline, on-device inference and embeddings into
your KMP apps, whether you’re building an AI assistant, a RAG chatbot, or an edge intelligence tool.</p>
<p>⸻</p>
<p>✨ Why Llamatik?</p>
<p>While the AI ecosystem is rich with APIs, most Kotlin developers are still tied to cloud-based
models. That means latency, privacy risks, and ongoing costs.</p>
<p>We believe the future of AI is:</p>
<p>• 🔐 Private: Your data stays on-device.</p>
<p>• 📱 Multiplatform: One codebase for Android, iOS, macOS, Linux, and Windows.</p>
<p>• ⚡ Performant: Built for small, quantized models that run fast on consumer hardware.</p>
<p>Llamatik bridges the gap between llama.cpp’s C++ backend and Kotlin’s modern multiplatform tooling —
giving you total control over your models and your data.</p>
<p>⸻</p>
<p>💡 What Can You Do with It?</p>
<p>With Llamatik, you can:</p>
<p>• 🧠 Run quantized LLMs like Phi-2, Mistral, or TinyLlama completely offline.</p>
<p>• 🔍 Generate embeddings using models like nomic-embed-text or bge-small locally.</p>
<p>• ⚡ Launch your own HTTP inference server using Ktor — fully self-contained, built into Llamatik,
and powered by llama.cpp.</p>
<p>• 🌐 Connect to remote llama.cpp endpoints (like llama-cpp-python, llama-server, or Ollama) with the
bundled HTTP client.</p>
<p>• 🔁 Use the same API across Android, iOS, and other native platforms — no platform-specific code
needed.</p>
<p>⸻</p>
<p>🌐 Built-in HTTP Inference Server</p>
<p>Llamatik includes a ready-to-use Ktor-based HTTP server that wraps your local llama.cpp models. You
can spin it up with a single call and expose endpoints like:</p>
<p>POST /v1/chat/completions</p>
<p>POST /v1/embeddings</p>
<p>GET /v1/models</p>
<p>This makes it easy to:</p>
<p>• 🔗 Connect your own apps (or other devices) to a shared local model</p>
<p>• 🧪 Use OpenAI-compatible tooling (e.g. LangChain, LlamaIndex) with your local server</p>
<p>• ⚙️ Integrate into your own edge deployments or experiments</p>
<p>No Python needed. No Docker required. Just Kotlin + Llamatik.</p>
<p>⸻</p>
<p>📦 What’s Inside?</p>
<p>• 🦙 Native bindings to llama.cpp for Kotlin/Native targets (no JNI or JNI-only limitations).</p>
<p>• 🧠 Multi-model context manager (for simultaneous generation + embeddings).</p>
<p>• 🛰️ Optional HTTP client and server.</p>
<p>• 🧱 Model loading, prompt building, and memory-efficient execution tailored for mobile.</p>
<p>• 🛠️ Simple, extensible API — built for Kotlin, not adapted from Python.</p>
<p>⸻</p>
<p>📢 Get Involved</p>
<p>Llamatik is open-source and community-driven. Whether you’re building AI-first apps or simply
exploring what’s possible offline, we’d love to hear from you.</p>
<p>🔗 <a href="https://github.com/ferranpons/Llamatik">GitHub Repo</a></p>

</div>





        
        <div class="container">
    <hr>
</div>
<div class="container has-text-centered top-pad">
    <a href="#top">
        <i class="fa fa-arrow-up"></i>
    </a>
</div>

<div class="container">
    <hr>
</div>

        <div class="section" id="footer">
    <div class="container has-text-centered">
        
        <span class="footer-text">
            <a href="https://github.com/victoriadrake/hugo-theme-introduction/"><strong>Introduction</strong></a> theme for <a href="http://gohugo.io/">Hugo</a>. Made with <a href="https://victoria.dev"><i class="fa fa-heart"></i> and <i class="fa fa-coffee"></i></a> by open source contributors.
        </span>
        
    </div>
</div>

        
    </div>
</section>




<script src="http://localhost:1313/js/bundle.5c23c0437f001a469ca373a465a6f7487203d18e10cdff76d86a60af66d5ee28.js" integrity="sha256-XCPAQ38AGkaco3OkZab3SHID0Y4Qzf922Gpgr2bV7ig="></script>








</body>
</html>
