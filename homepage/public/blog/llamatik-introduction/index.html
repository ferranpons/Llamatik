<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
    


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="Kotlin-first llama.cpp integration for on-device and remote LLM inference.">
<title>
    â€œ Introducing Llamatik Offline Llms for Kotlin Multiplatformâ€ - Llamatik
</title>



    
    <meta property="og:title"
      content="â€œIntroducing Llamatik Offline LLMs for Kotlin Multiplatformâ€ - Llamatik"/>
<meta property="og:type" content="website"/>
<meta property="og:description"
      content="â€œMeet Llamatik â€” a multiplatform Kotlin library to run llama.cpp locally on Android, iOS, and desktop, complete with an HTTP server.â€"/>
<meta property="og:url" content="http://localhost:1313/blog/llamatik-introduction/"/>
<meta property="og:site_name" content="Llamatik"/>




<meta property="og:image" content="http://localhost:1313/home/llamatik-icon-logo.png"/>

<meta property="og:image" content="http://localhost:1313/home/profile.jpg"/>




    
<link rel="shortcut icon" href="/img/fav.ico">


    




<link rel="stylesheet" href="/css/main.css" media="screen">





    
    
    
    
</head>
<body>
<section id="top" class="section">
    
    <div class="container hero  fade-in one ">
        

<h1 class="bold-title is-1">Llamatik Blog</h1>


    </div>
    
    <div class="section  fade-in two ">
        

<div class="container">
    <hr>
    <nav class="navbar" role="navigation" aria-label="main navigation">
        
        <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu"
           aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
        <div class="navbar-menu " id="navMenu">
            
            
            
            
            <a class="navbar-item" href="/">main</a>
            

            
            

            
            
            

            

            
            
            
            
            
            <a class="navbar-item"
               href="/%20#about">About</a>
            
            
            
            
            
            
            
            
            
            <a class="navbar-item" href="http://localhost:1313/blog/">
                
                Back to Llamatik Blog
                
            </a>
            
            
            
            
            
            

            
            
            <a class="navbar-item"
               href="/%20#contact">Contact</a>
            
            

            
            
            

            
            <a class="navbar-item" href=""><i class="fas fa-rss"></i></a>
            
            
        </div>
    </nav>
    <hr>
</div>




        
<div class="container">
    <h2 class="title is-1 top-pad strong-post-title">
        <a href="http://localhost:1313/blog/llamatik-introduction/">â€œIntroducing Llamatik Offline LLMs for Kotlin Multiplatformâ€</a>
    </h2>
    
    <div class="post-data">
        Jul 22, 2025
        
         | 
        3 minutes read
        
    </div>
    
    <div class="blog-share">
        Share this:
        
        <a class="twitter-share-button"
           href="https://twitter.com/intent/tweet?text=%e2%80%9cIntroducing%20Llamatik%20Offline%20LLMs%20for%20Kotlin%20Multiplatform%e2%80%9d%20http%3a%2f%2flocalhost%3a1313%2fblog%2fllamatik-introduction%2f"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
            <i class="fab fa-twitter"></i>
            <span class="hidden">Twitter</span>
        </a>
        
        
        
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <p>
        Tags:
        
        <a href="/tags/kotlin">
        Kotlin</a>,
        
        <a href="/tags/multiplatform">
        Multiplatform</a>,
        
        <a href="/tags/llm">
        LLM</a>,
        
        <a href="/tags/llama.cpp">
        llama.cpp</a>,
        
        <a href="/tags/offline-ai">
        Offline AI</a>
        
    </p>
    
    
    
    
</div>

<div class="container markdown top-pad">
    <p>ğŸ¦™ Introducing Llamatik: Offline LLMs for Kotlin Multiplatform</p>
<p>Weâ€™re thrilled to introduce Llamatik â€” an open-source Kotlin Multiplatform library that brings local
Large Language Models (LLMs) to Android, iOS, desktop, and beyond using the power of llama.cpp.</p>
<p>Llamatik makes it simple and efficient to integrate offline, on-device inference and embeddings into
your KMP apps, whether youâ€™re building an AI assistant, a RAG chatbot, or an edge intelligence tool.</p>
<p>â¸»</p>
<p>âœ¨ Why Llamatik?</p>
<p>While the AI ecosystem is rich with APIs, most Kotlin developers are still tied to cloud-based
models. That means latency, privacy risks, and ongoing costs.</p>
<p>We believe the future of AI is:</p>
<p>â€¢ ğŸ” Private: Your data stays on-device.</p>
<p>â€¢ ğŸ“± Multiplatform: One codebase for Android, iOS, macOS, Linux, and Windows.</p>
<p>â€¢ âš¡ Performant: Built for small, quantized models that run fast on consumer hardware.</p>
<p>Llamatik bridges the gap between llama.cppâ€™s C++ backend and Kotlinâ€™s modern multiplatform tooling â€”
giving you total control over your models and your data.</p>
<p>â¸»</p>
<p>ğŸ’¡ What Can You Do with It?</p>
<p>With Llamatik, you can:</p>
<p>â€¢ ğŸ§  Run quantized LLMs like Phi-2, Mistral, or TinyLlama completely offline.</p>
<p>â€¢ ğŸ” Generate embeddings using models like nomic-embed-text or bge-small locally.</p>
<p>â€¢ âš¡ Launch your own HTTP inference server using Ktor â€” fully self-contained, built into Llamatik,
and powered by llama.cpp.</p>
<p>â€¢ ğŸŒ Connect to remote llama.cpp endpoints (like llama-cpp-python, llama-server, or Ollama) with the
bundled HTTP client.</p>
<p>â€¢ ğŸ” Use the same API across Android, iOS, and other native platforms â€” no platform-specific code
needed.</p>
<p>â¸»</p>
<p>ğŸŒ Built-in HTTP Inference Server</p>
<p>Llamatik includes a ready-to-use Ktor-based HTTP server that wraps your local llama.cpp models. You
can spin it up with a single call and expose endpoints like:</p>
<p>POST /v1/chat/completions</p>
<p>POST /v1/embeddings</p>
<p>GET /v1/models</p>
<p>This makes it easy to:</p>
<p>â€¢ ğŸ”— Connect your own apps (or other devices) to a shared local model</p>
<p>â€¢ ğŸ§ª Use OpenAI-compatible tooling (e.g. LangChain, LlamaIndex) with your local server</p>
<p>â€¢ âš™ï¸ Integrate into your own edge deployments or experiments</p>
<p>No Python needed. No Docker required. Just Kotlin + Llamatik.</p>
<p>â¸»</p>
<p>ğŸ“¦ Whatâ€™s Inside?</p>
<p>â€¢ ğŸ¦™ Native bindings to llama.cpp for Kotlin/Native targets (no JNI or JNI-only limitations).</p>
<p>â€¢ ğŸ§  Multi-model context manager (for simultaneous generation + embeddings).</p>
<p>â€¢ ğŸ›°ï¸ Optional HTTP client and server.</p>
<p>â€¢ ğŸ§± Model loading, prompt building, and memory-efficient execution tailored for mobile.</p>
<p>â€¢ ğŸ› ï¸ Simple, extensible API â€” built for Kotlin, not adapted from Python.</p>
<p>â¸»</p>
<p>ğŸ“¢ Get Involved</p>
<p>Llamatik is open-source and community-driven. Whether youâ€™re building AI-first apps or simply
exploring whatâ€™s possible offline, weâ€™d love to hear from you.</p>
<p>ğŸ”— <a href="https://github.com/ferranpons/Llamatik">GitHub Repo</a></p>

</div>





        
        <div class="container">
    <hr>
</div>
<div class="container has-text-centered top-pad">
    <a href="#top">
        <i class="fa fa-arrow-up"></i>
    </a>
</div>

<div class="container">
    <hr>
</div>

        <div class="section" id="footer">
    <div class="container has-text-centered">
        
        <span class="footer-text">
            <a href="https://github.com/victoriadrake/hugo-theme-introduction/"><strong>Introduction</strong></a> theme for <a href="http://gohugo.io/">Hugo</a>. Made with <a href="https://victoria.dev"><i class="fa fa-heart"></i> and <i class="fa fa-coffee"></i></a> by open source contributors.
        </span>
        
    </div>
</div>

        
    </div>
</section>




<script src="http://localhost:1313/js/bundle.5c23c0437f001a469ca373a465a6f7487203d18e10cdff76d86a60af66d5ee28.js" integrity="sha256-XCPAQ38AGkaco3OkZab3SHID0Y4Qzf922Gpgr2bV7ig="></script>








</body>
</html>
